import csv
import argparse
import sys
import os
import tempfile
import glob
from datetime import datetime, timedelta
from utils import setup_logger, get_engine_from_env, get_src_dir

# -----------------------------------------------------------
# âš™ï¸ ì•ˆì „í•œ ë³€í™˜ í•¨ìˆ˜
# -----------------------------------------------------------
def safe_float(x):
    try:
        return float(x)
    except (TypeError, ValueError):
        return 0.0

def normalize_date(etl_str: str) -> str:
    etl_str = etl_str.strip()

    if "-" in etl_str:
        return etl_str  # ì´ë¯¸ YYYY-MM-DD

    if len(etl_str) == 8:
        return datetime.strptime(etl_str, "%Y%m%d").strftime("%Y-%m-%d")

    if len(etl_str) == 6:
        return datetime.strptime(etl_str + "01", "%Y%m%d").strftime("%Y-%m-%d")

    raise ValueError(f"Unknown etl_ymd format: {etl_str}")

def ensure_parent_table(cur):
    """
    public.tb_flowpop ë¶€ëª¨ í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìë™ ìƒì„±.
    ìˆìœ¼ë©´ ì•„ë¬´ ì‘ì—…ë„ í•˜ì§€ ì•ŠìŒ.
    """

    # 1) ë¶€ëª¨ í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ ì²´í¬
    cur.execute("""
        SELECT EXISTS (
            SELECT 1
            FROM information_schema.tables 
            WHERE table_schema='public' 
              AND table_name='tb_flowpop'
        );
    """)
    exists = cur.fetchone()[0]

    if exists:
        logger.info("âœ” ë¶€ëª¨ í…Œì´ë¸” tb_flowpop ì´ë¯¸ ì¡´ì¬")
        return

    logger.info("âš  ë¶€ëª¨ í…Œì´ë¸” tb_flowpop ì—†ìŒ â†’ ìë™ ìƒì„± ì‹œì‘")

    # 2) ë¶€ëª¨ í…Œì´ë¸” ìƒì„±
    create_sql = """
    CREATE TABLE IF NOT EXISTS public.tb_flowpop (
        id           bpchar(8),
        "type"       varchar(20),
        timezn_cd    varchar(10),
        m10 float8,
        m20 float8,
        m30 float8,
        m40 float8,
        m50 float8,
        m60 float8,
        m70 float8,
        f10 float8,
        f20 float8,
        f30 float8,
        f40 float8,
        f50 float8,
        f60 float8,
        f70 float8,
        total float8,
        admi_cd varchar(20),
        etl_ymd date NOT NULL
    )
    PARTITION BY RANGE (etl_ymd);
    """

    cur.execute(create_sql)
    logger.info("ğŸ‰ ë¶€ëª¨ í…Œì´ë¸” tb_flowpop ìƒì„± ì™„ë£Œ")    

# -----------------------------------------------------------
# ğŸ“… ì›”ë³„ íŒŒí‹°ì…˜ ìë™ ìƒì„± í•¨ìˆ˜
# -----------------------------------------------------------
def ensure_partition(cur, etl_ymd_str):
    """etl_ymd ê°’(YYYYMMDD ë˜ëŠ” YYYY-MM-DD)ì„ ê¸°ì¤€ìœ¼ë¡œ ì›”ë³„ íŒŒí‹°ì…˜ ìƒì„±"""
    # ì…ë ¥ ë¬¸ìì—´ ì •ê·œí™”
    etl_ymd_str = etl_ymd_str.strip()

    # í˜•ì‹ ìë™ íŒë³„
    if "-" in etl_ymd_str:
        ymd = datetime.strptime(etl_ymd_str, "%Y-%m-%d").date()
    elif len(etl_ymd_str) == 8:
        ymd = datetime.strptime(etl_ymd_str, "%Y%m%d").date()
    elif len(etl_ymd_str) == 6:
        # YYYYMM í˜•íƒœë§Œ ìˆëŠ” ê²½ìš° (ì˜ˆ: 202501)
        ymd = datetime.strptime(etl_ymd_str + "01", "%Y%m%d").date()
    else:
        raise ValueError(f"Unknown date format: {etl_ymd_str}")

    start = ymd.replace(day=1)
    next_month = (start.replace(day=28) + timedelta(days=4)).replace(day=1)

    partition_name = f"public.tb_flowpop_{start.strftime('%Y%m')}"
    sql = f"""
    CREATE TABLE IF NOT EXISTS {partition_name}
        PARTITION OF public.tb_flowpop
        FOR VALUES FROM ('{start}') TO ('{next_month}');
    CREATE INDEX IF NOT EXISTS idx_tb_flowpop_{start.strftime('%Y%m')}_timezn_ymd
        ON {partition_name} (etl_ymd, timezn_cd, id);
    """
    cur.execute(sql)
    logger.info(f"ğŸ“¦ íŒŒí‹°ì…˜ í™•ì¸/ìƒì„± ì™„ë£Œ: {partition_name}")
    return partition_name

# -------------------------------------------------------------------
# ğŸ”§ ì§‘ê³„ í…Œì´ë¸” ìë™ ìƒì„± ê³µí†µ í•¨ìˆ˜
# -------------------------------------------------------------------
def ensure_table_exists(engine, table_name, create_sql):
    check_sql = f"""
        SELECT EXISTS (
            SELECT 1 FROM information_schema.tables
            WHERE table_schema='public' AND table_name='{table_name}'
        );
    """

    raw = engine.raw_connection()
    try:
        cur = raw.cursor()
        cur.execute(check_sql)
        exists = cur.fetchone()[0]

        if exists:
            logger.info(f"âœ” {table_name} ì´ë¯¸ ì¡´ì¬")
        else:
            logger.info(f"âš  {table_name} ì—†ìŒ â†’ ìƒì„±")
            cur.execute(create_sql)
            raw.commit()
            logger.info(f"ğŸ‰ {table_name} ìƒì„± ì™„ë£Œ")

    finally:
        cur.close()
        raw.close()

# -------------------------------------------------------------------
# ğŸ“Š ì§‘ê³„ í…Œì´ë¸” ìƒì„± SQL
# -------------------------------------------------------------------
CREATE_AGG_AGEGEN = """
CREATE TABLE public.tb_flowpop_agg_agegen (
    crtr_ym varchar(6),
    type varchar(20),
    gender varchar(1),
    age varchar(3),
    total_population numeric
);
"""

CREATE_AGG_WEEKDAY = """
CREATE TABLE public.tb_flowpop_agg_timezn (
    crtr_ym varchar(6),
    timezn_cd varchar(10),
    type varchar(20),
    total_population numeric
);
"""

CREATE_AGG_TMZONE = """
CREATE TABLE public.tb_flowpop_agg_dayname (
    crtr_ym varchar(6),
    dayname varchar(10),
    type varchar(20),
    total_population numeric
);
"""

CREATE_AGG_DAILY = """
CREATE TABLE public.tb_flowpop_agg_daily (
    crtr_ym varchar(6),
    admi_cd varchar(20),
    etl_ymd date,
    total_population numeric
);
"""

# -----------------------------------------------------------
# ğŸ“Š SQL ê¸°ë°˜ ì§‘ê³„ ìƒì„± í•¨ìˆ˜
# -----------------------------------------------------------

def run_sql_aggregations(ym, engine):
    # ì§‘ê³„ í…Œì´ë¸” ìë™ ìƒì„±
    ensure_table_exists(engine, "tb_flowpop_agg_agegen", CREATE_AGG_AGEGEN)
    ensure_table_exists(engine, "tb_flowpop_agg_timezn", CREATE_AGG_WEEKDAY)
    ensure_table_exists(engine, "tb_flowpop_agg_dayname", CREATE_AGG_TMZONE)
    ensure_table_exists(engine, "tb_flowpop_agg_daily", CREATE_AGG_DAILY)

    tn = f"public.tb_flowpop_{ym}"

    start_date = datetime.strptime(ym, "%Y%m").date()
    if start_date.month == 12:
        next_date = start_date.replace(year=start_date.year + 1, month=1, day=1)
    else:
        next_date = start_date.replace(month=start_date.month + 1, day=1)

    # ë‚ ì§œ ë¬¸ìì—´ ë³€í™˜
    start_s = start_date.strftime("%Y-%m-%d")
    next_s  = next_date.strftime("%Y-%m-%d")

    # ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ 
    sql_dict = {
        "tb_flowpop_agg_agegen" : f"""
        INSERT INTO tb_flowpop_agg_agegen (crtr_ym, type, gender, age, total_population)
        WITH agg AS (
            SELECT 
                type,
                SUM(m10) AS m10, SUM(m20) AS m20, SUM(m30) AS m30, SUM(m40) AS m40,
                SUM(m50) AS m50, SUM(m60) AS m60, SUM(m70) AS m70,
                SUM(f10) AS f10, SUM(f20) AS f20, SUM(f30) AS f30, SUM(f40) AS f40,
                SUM(f50) AS f50, SUM(f60) AS f60, SUM(f70) AS f70
            FROM {tn}
            WHERE etl_ymd >= '{start_s}' AND etl_ymd < '{next_s}'
            GROUP BY type
        ),
        unpivot AS (
            SELECT 
                type,
                gender[i] AS gender,
                age[i] AS age,
                population[i] AS total_population
            FROM (
                SELECT
                    type,
                    ARRAY['M','M','M','M','M','M','M',
                        'F','F','F','F','F','F','F'] AS gender,
                    ARRAY['10','20','30','40','50','60','70',
                        '10','20','30','40','50','60','70'] AS age,
                    ARRAY[
                        m10, m20, m30, m40, m50, m60, m70,
                        f10, f20, f30, f40, f50, f60, f70
                    ] AS population
                FROM agg
            ) t,
            generate_subscripts(gender, 1) AS i
        )
        SELECT  
            '{ym}' AS crtr_ym,
            type,
            gender,
            age,
            ROUND(total_population::numeric, 2)
        FROM unpivot;
        """,

        "tb_flowpop_agg_timezn":f"""
        INSERT INTO tb_flowpop_agg_timezn (crtr_ym, timezn_cd, type, total_population)
        SELECT '{ym}', timezn_cd, type, ROUND(SUM(total)::numeric,2)
        FROM {tn}
        WHERE etl_ymd >= '{start_s}' AND etl_ymd < '{next_s}'
        GROUP BY timezn_cd, type;
        """,

        "tb_flowpop_agg_dayname":f"""
        INSERT INTO tb_flowpop_agg_dayname (crtr_ym, dayname, type, total_population)
        SELECT '{ym}', to_char(etl_ymd,'dy') as dayname, type, ROUND(SUM(total)::numeric,2)
        FROM {tn}
        WHERE etl_ymd >= '{start_s}' AND etl_ymd < '{next_s}'
        GROUP BY to_char(etl_ymd,'dy'), type;
        """,

        "tb_flowpop_agg_daily":f"""
        INSERT INTO tb_flowpop_agg_daily (crtr_ym, admi_cd, etl_ymd, total_population)
        SELECT '{ym}', admi_cd, etl_ymd, ROUND(SUM(total)::numeric,2)
        FROM {tn}
        WHERE etl_ymd >= '{start_s}' AND etl_ymd < '{next_s}'
        GROUP BY admi_cd, etl_ymd;
        """
    }

    # psycopg2 raw cursor ì‚¬ìš©
    raw = engine.raw_connection()
    try:
        cur = raw.cursor()

        for name, query in sql_dict.items():
            logger.info(f"â–¶ [ì§‘ê³„ ì‹¤í–‰ ì‹œì‘] {name}")
            cur.execute(query)
            logger.info(f"âœ” [ì§‘ê³„ ì‹¤í–‰ ì™„ë£Œ] {name}")

        raw.commit()
        logger.info(f"ğŸ“Š ì „ì²´ SQL ì§‘ê³„ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {ym}")

    except Exception as e:
        raw.rollback()
        logger.error(f"âŒ ì§‘ê³„ ì‹¤í–‰ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise e

    finally:
        cur.close()
        raw.close()

# -----------------------------------------------------------
# ğŸš€ ë©”ì¸ ETL ë¡œì§
# -----------------------------------------------------------
def load_flowpop(input_file):
    logger.info(f"ì‹œì‘: {input_file} íŒŒì¼ì„ PostgreSQLë¡œ ì ì¬í•©ë‹ˆë‹¤.")

    engine = get_engine_from_env()
    conn = engine.raw_connection()
    cur = conn.cursor()

    columns_to_exclude = [
        'x', 'y',
        'm00', 'm15', 'm25', 'm35', 'm45', 'm55', 'm65',
        'f00', 'f15', 'f25', 'f35', 'f45', 'f55', 'f65',
    ]

    with tempfile.NamedTemporaryFile(mode='w+', delete=False) as temp_file:

        reader = csv.DictReader(open(input_file, 'r', encoding='utf-8', newline=''), delimiter='|')
        all_columns = reader.fieldnames
        selected_columns = [c for c in all_columns if c not in columns_to_exclude]
        final_columns = selected_columns

        writer = csv.writer(temp_file, delimiter=',')

        first_etl_ymd = None
        row_count = 0

        for row in reader:
            row['etl_ymd'] = normalize_date(row['etl_ymd'])

            if first_etl_ymd is None:
                first_etl_ymd = row['etl_ymd']

            # ì—°ë ¹/ì„±ë³„ í•©ì‚°
            row['m10'] = safe_float(row['m00']) + safe_float(row['m10']) + safe_float(row['m15'])
            row['m20'] = safe_float(row['m20']) + safe_float(row['m25'])
            row['m30'] = safe_float(row['m30']) + safe_float(row['m35'])
            row['m40'] = safe_float(row['m40']) + safe_float(row['m45'])
            row['m50'] = safe_float(row['m50']) + safe_float(row['m55'])
            row['m60'] = safe_float(row['m60']) + safe_float(row['m65'])

            row['f10'] = safe_float(row['f00']) + safe_float(row['f10']) + safe_float(row['f15'])
            row['f20'] = safe_float(row['f20']) + safe_float(row['f25'])
            row['f30'] = safe_float(row['f30']) + safe_float(row['f35'])
            row['f40'] = safe_float(row['f40']) + safe_float(row['f45'])
            row['f50'] = safe_float(row['f50']) + safe_float(row['f55'])
            row['f60'] = safe_float(row['f60']) + safe_float(row['f65'])

            row['admi_cd'] = str(int(row['admi_cd']) * 100)

            for c in columns_to_exclude:
                row.pop(c, None)

            writer.writerow([row[c] for c in final_columns])
            row_count += 1

            if row_count % 5000000 == 0:
                logger.info(f"ì§„í–‰ ì¤‘: {row_count:,}í–‰ ì²˜ë¦¬ ì™„ë£Œ")

        temp_file.flush()

    if not first_etl_ymd:
        logger.error("âŒ etl_ymd ê°’ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info(f"ì´ {row_count:,}í–‰ ë³€í™˜ ì™„ë£Œ (etl_ymd={first_etl_ymd})")
    ensure_parent_table(cur)
    partition_name = ensure_partition(cur, first_etl_ymd)

    with open(temp_file.name, 'r') as temp_file_read:
        logger.info(f"COPY ì‹œì‘ â†’ {partition_name}")
        cur.copy_expert(
            f"""
            COPY {partition_name} ({', '.join(final_columns)})
            FROM STDIN WITH (FORMAT CSV)
            """,
            temp_file_read
        )

    conn.commit()
    cur.close()
    conn.close()

    logger.info(f"âœ… ë°ì´í„° ì ì¬ ì™„ë£Œ: {partition_name}, ì´ {row_count:,}í–‰")


# -----------------------------------------------------------
# ğŸ§­ ì‹¤í–‰ë¶€
# -----------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="FLOWPOP ì›”ë³„ ë°ì´í„° ì ì¬ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("ym", help="ì ì¬í•  ì›”(YYYYMM)")
    logger = setup_logger("flowpop")

    args = parser.parse_args()
    logger.info("â–¶ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")

    SRC_DIR = get_src_dir()
    pattern = f"*flow_age_time*{args.ym}*.csv"
    matched_files = sorted(glob.glob(os.path.join(SRC_DIR, pattern)))

    if not matched_files:
        logger.error(f"âŒ {args.ym}ì´ í¬í•¨ëœ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    input_file = matched_files[-1]
    logger.info(f"ì„ íƒëœ íŒŒì¼: {input_file}")

    try:
        load_flowpop(input_file)
        run_sql_aggregations(args.ym, get_engine_from_env())
        logger.info("â–¶ ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ")
    except Exception as e:
        logger.exception(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)
