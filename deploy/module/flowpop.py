import csv
from io import StringIO
import argparse
import sys
import datetime
import os
import tempfile
import glob
from utils import setup_logger, get_engine_from_env, get_src_dir

# -----------------------------------------------------------
# âš™ï¸ ì•ˆì „í•œ ë³€í™˜ í•¨ìˆ˜
# -----------------------------------------------------------
def safe_float(x):
    try:
        return float(x)
    except (TypeError, ValueError):
        return 0.0

def normalize_date(etl_str: str) -> str:
    """etl_ymd ë¬¸ìì—´ì„ YYYY-MM-DD í˜•íƒœë¡œ ë³€í™˜"""
    etl_str = etl_str.strip()
    if "-" in etl_str:
        return etl_str  # ì´ë¯¸ YYYY-MM-DD
    elif len(etl_str) == 8:
        return datetime.datetime.strptime(etl_str, "%Y%m%d").strftime("%Y-%m-%d")
    elif len(etl_str) == 6:
        return datetime.datetime.strptime(etl_str + "01", "%Y%m%d").strftime("%Y-%m-%d")
    else:
        raise ValueError(f"Unknown etl_ymd format: {etl_str}")
    
# -----------------------------------------------------------
# ğŸ“… ì›”ë³„ íŒŒí‹°ì…˜ ìë™ ìƒì„± í•¨ìˆ˜
# -----------------------------------------------------------
def ensure_partition(cur, etl_ymd_str):
    """etl_ymd ê°’(YYYYMMDD ë˜ëŠ” YYYY-MM-DD)ì„ ê¸°ì¤€ìœ¼ë¡œ ì›”ë³„ íŒŒí‹°ì…˜ ìƒì„±"""
    # ì…ë ¥ ë¬¸ìì—´ ì •ê·œí™”
    etl_ymd_str = etl_ymd_str.strip()

    # í˜•ì‹ ìë™ íŒë³„
    if "-" in etl_ymd_str:
        ymd = datetime.datetime.strptime(etl_ymd_str, "%Y-%m-%d").date()
    elif len(etl_ymd_str) == 8:
        ymd = datetime.datetime.strptime(etl_ymd_str, "%Y%m%d").date()
    elif len(etl_ymd_str) == 6:
        # YYYYMM í˜•íƒœë§Œ ìˆëŠ” ê²½ìš° (ì˜ˆ: 202501)
        ymd = datetime.datetime.strptime(etl_ymd_str + "01", "%Y%m%d").date()
    else:
        raise ValueError(f"Unknown date format: {etl_ymd_str}")

    start = ymd.replace(day=1)
    next_month = (start.replace(day=28) + datetime.timedelta(days=4)).replace(day=1)

    partition_name = f"public.tb_flowpop_{start.strftime('%Y%m')}"
    sql = f"""
    CREATE TABLE IF NOT EXISTS {partition_name}
        PARTITION OF public.tb_flowpop
        FOR VALUES FROM ('{start}') TO ('{next_month}');
    CREATE INDEX IF NOT EXISTS idx_tb_flowpop_{start.strftime('%Y%m')}_timezn_ymd
        ON {partition_name} (etl_ymd, timezn_cd, id);
    """
    cur.execute(sql)
    logger.info(f"ğŸ“¦ íŒŒí‹°ì…˜ í™•ì¸/ìƒì„± ì™„ë£Œ: {partition_name}")
    return partition_name

# -----------------------------------------------------------
# ğŸš€ ë©”ì¸ ETL ë¡œì§
# -----------------------------------------------------------
def load_flowpop(input_file):
    logger.info(f"ì‹œì‘: {input_file} íŒŒì¼ì„ PostgreSQLë¡œ ì ì¬í•©ë‹ˆë‹¤.")

    engine = get_engine_from_env()
    conn = engine.raw_connection()
    cur = conn.cursor()

    # -----------------------------------------------------------
    # âš™ï¸ ì œê±°í•  ì»¬ëŸ¼ ëª©ë¡
    # -----------------------------------------------------------
    columns_to_exclude = [
        'x', 'y',
        'm00', 'm15', 'm25', 'm35', 'm45', 'm55', 'm65',
        'f00', 'f15', 'f25', 'f35', 'f45', 'f55', 'f65',
        'admi_cd'
    ]

    # -----------------------------------------------------------
    # ğŸ“¥ CSV ì½ê¸°
    # -----------------------------------------------------------
    with tempfile.NamedTemporaryFile(mode='w+', delete=False) as temp_file:
        reader = csv.DictReader(open(input_file, 'r', encoding='utf-8', newline=''), delimiter='|')
        all_columns = reader.fieldnames
        selected_columns = [c for c in all_columns if c not in columns_to_exclude]
        final_columns = selected_columns
        writer = csv.writer(temp_file, delimiter=',')

        first_etl_ymd = None
        row_count = 0

        for row in reader:
            # etl_ymd ì¶”ì¶œ (ìµœì´ˆ í•œ ë²ˆë§Œ)
            row['etl_ymd'] = normalize_date(row['etl_ymd'])
            if first_etl_ymd is None:
                first_etl_ymd = row['etl_ymd']

            # ì„±ë³„Â·ì—°ë ¹ëŒ€ í•©ì‚° (float ê¸°ë°˜)
            row['m10'] = safe_float(row['m00']) + safe_float(row['m10']) + safe_float(row['m15'])
            row['m20'] = safe_float(row['m20']) + safe_float(row['m25'])
            row['m30'] = safe_float(row['m30']) + safe_float(row['m35'])
            row['m40'] = safe_float(row['m40']) + safe_float(row['m45'])
            row['m50'] = safe_float(row['m50']) + safe_float(row['m55'])
            row['m60'] = safe_float(row['m60']) + safe_float(row['m65'])

            row['f10'] = safe_float(row['f00']) + safe_float(row['f10']) + safe_float(row['f15'])
            row['f20'] = safe_float(row['f20']) + safe_float(row['f25'])
            row['f30'] = safe_float(row['f30']) + safe_float(row['f35'])
            row['f40'] = safe_float(row['f40']) + safe_float(row['f45'])
            row['f50'] = safe_float(row['f50']) + safe_float(row['f55'])
            row['f60'] = safe_float(row['f60']) + safe_float(row['f65'])

            # í•„ìš” ì—†ëŠ” ì»¬ëŸ¼ ì œê±°
            for c in columns_to_exclude:
                row.pop(c, None)

            # CSV ë²„í¼ ê¸°ë¡
            writer.writerow([row[c] for c in final_columns])

            row_count += 1
            if row_count % 5000000 == 0:
                logger.info(f"ì§„í–‰ ì¤‘: {row_count:,}í–‰ ì²˜ë¦¬ ì™„ë£Œ")

        temp_file.flush()

    if not first_etl_ymd:
        logger.error("âŒ etl_ymd ê°’ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CSV êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    logger.info(f"ì´ {row_count:,}í–‰ ë³€í™˜ ì™„ë£Œ (etl_ymd={first_etl_ymd})")

    # -----------------------------------------------------------
    # ğŸ§± íŒŒí‹°ì…˜ ìë™ ìƒì„±
    # -----------------------------------------------------------
    partition_name = ensure_partition(cur, first_etl_ymd)

    # -----------------------------------------------------------
    # ğŸ“¨ COPY ìˆ˜í–‰
    # -----------------------------------------------------------
    with open(temp_file.name, 'r') as temp_file_read:
        logger.info(f"PostgreSQL COPY ì‹œì‘ â†’ {partition_name}")
        cur.copy_expert(f"""
            COPY {partition_name} ({', '.join(final_columns)})
            FROM STDIN WITH (FORMAT CSV)
        """, temp_file_read)

    conn.commit()
    cur.close()
    conn.close()

    logger.info(f"âœ… ë°ì´í„° ì ì¬ ì™„ë£Œ: {partition_name}, ì´ {row_count:,}í–‰")

# -----------------------------------------------------------
# ğŸ§­ ì‹¤í–‰ë¶€
# -----------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="FLOWPOP ì›”ë³„ ë°ì´í„° ì ì¬ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--ym", required=True, help="ì ì¬í•  ì›”(YYYYMM)")
    logger = setup_logger("flowpop")
    logger.info("â–¶ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    args = parser.parse_args()

    # ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ 
    SRC_DIR = get_src_dir()
    pattern = f"*flow_age_time*{args.ym}*.csv"
    matched_files = sorted(glob.glob(os.path.join(SRC_DIR, pattern)))

    # íŒŒì¼ íƒìƒ‰
    if not matched_files:
        logger.error(f"âŒ {args.ym}ì´(ê°€) í¬í•¨ëœ CSV íŒŒì¼ì„ {SRC_DIR}ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    input_file = matched_files[-1]
    logger.info(f"ì„ íƒëœ íŒŒì¼: {input_file}")
    try:
        load_flowpop(input_file)
    except Exception as e:
        logger.exception(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)
