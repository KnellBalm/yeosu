import joblib
import json
import logging
import pandas as pd
import numpy as np
from sqlalchemy import create_engine
from dotenv import load_dotenv, find_dotenv
import os
from datetime import datetime, timedelta

# .env íŒŒì¼ ë¡œë“œ
bundle_path = "/DATA/jupyter_WorkingDirectory/notebook/yeosu/deploy/module/predict_model/xgb_quantile_bundle.joblib"
metadata_path = "/DATA/jupyter_WorkingDirectory/notebook/yeosu/deploy/module/predict_model/xgb_quantile_metadata.json"
grid_mapping_path = "/DATA/jupyter_WorkingDirectory/notebook/yeosu/deploy/data/json/wifi_grid_id.json"

env_path = find_dotenv(usecwd=True)
if not env_path:
    env_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".env"))
load_dotenv(env_path)
# ============================================
# ğŸª¶ ë¡œê¹… ì„¤ì •
# ============================================
def setup_logger(script_name):
    log_dir = os.getenv("LOG_DIR", "./logs")  # ê¸°ë³¸ ë¡œê·¸ ë””ë ‰í† ë¦¬
    log_file_path = os.path.join(log_dir, f"{script_name}.log")  # íŒŒì¼ëª… ë™ì  ì„¤ì •

    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)

    logger = logging.getLogger(script_name)
    if not logger.handlers:  # ì¤‘ë³µ ë°©ì§€
        logging.basicConfig(
            filename=log_file_path,
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S"
        )
        console = logging.StreamHandler()
        console.setLevel(logging.INFO)
        formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
        console.setFormatter(formatter)
        logger.addHandler(console)
        logger.info("ğŸ“˜ Logging initialized.")
    return logger

logger = setup_logger("wifi_predict")

# ============================================
# PostgreSQL ì—°ê²° ìƒì„±
# ============================================
def get_source_engine():
    db_config = {
        "DB_USER": os.getenv("WIFI_DB_USER"),
        "DB_PASS": os.getenv("WIFI_DB_PASS"),
        "DB_HOST": os.getenv("WIFI_DB_HOST"),
        "DB_PORT": os.getenv("WIFI_DB_PORT"),
        "DB_NAME": os.getenv("WIFI_DB_NAME")
    }

    url = (
        f"postgresql+psycopg2://{db_config['DB_USER']}:{db_config['DB_PASS']}"
        f"@{db_config['DB_HOST']}:{db_config['DB_PORT']}/{db_config['DB_NAME']}"
    )
    return create_engine(url)

def get_target_engine():
    db_config = {
        "DB_USER": os.getenv("DB_USER"),
        "DB_PASS": os.getenv("DB_PASS"),
        "DB_HOST": os.getenv("DB_HOST"),
        "DB_PORT": os.getenv("DB_PORT"),
        "DB_NAME": os.getenv("DB_NAME")
    }

    url = (
        f"postgresql+psycopg2://{db_config['DB_USER']}:{db_config['DB_PASS']}"
        f"@{db_config['DB_HOST']}:{db_config['DB_PORT']}/{db_config['DB_NAME']}"
    )
    return create_engine(url)

source_engine = get_source_engine()
target_engine = get_target_engine()

# ============================================
# ğŸ“˜ ì €ì¥ëœ ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ë¡œë“œ
# ============================================
bundle = joblib.load(bundle_path)
model = bundle["model"]
le = bundle["label_encoder"]
numeric_features = bundle["numeric_features"]
categorical_features = bundle["categorical_features"]

with open(metadata_path, "r") as f:
    meta = json.load(f)

logger.info(f"âœ… ëª¨ë¸ ë²„ì „ ë¡œë“œ ì™„ë£Œ ({meta['trained_at']})")

# ============================================
# ğŸ“¥ ì‹ ê·œ ë°ì´í„° ì¤€ë¹„
# ============================================
# í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ std_date ë™ì  ìƒì„±
today = datetime.now()
start_date = (today - timedelta(days=60)).strftime('%Y-%m-%d')  # 30ì¼ ì „ë¶€í„° ì‹œì‘
logger.info(f"ğŸ“… ë™ì  ë‚ ì§œ ì„¤ì •: {start_date} ì´í›„ ë°ì´í„° ë¡œë“œ")

query = f"""
        SELECT ap_id, std_date, cnt AS acs_cnt
        FROM ap.log_summary
        WHERE LEFT(std_date, 10)::date >= '{start_date}'
        """
logger.debug(f"{query=}")
new_data = pd.read_sql_query(query, source_engine)
logger.info(f"âœ… ì‹ ê·œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ : {len(new_data):,} rows")

# ì™€ì´íŒŒì´ - ê²©ì ë§¤í•‘ ID ê°€ì ¸ì˜¤ê¸°
wifi_grid_id = json.load(open(grid_mapping_path, "r"))

new_data['grid_id'] = new_data['ap_id'].map(wifi_grid_id)
new_data['std_date'] = pd.to_datetime(new_data['std_date'])
new_data = new_data.groupby(['grid_id', 'std_date'], as_index=False).agg(acs_cnt=('acs_cnt', 'sum'))

new_data['month'] = new_data['std_date'].dt.month
new_data['dayname'] = new_data['std_date'].dt.day_name()
new_data['hour'] = new_data['std_date'].dt.hour
new_data['is_weekend_group'] = new_data['dayname'].isin(["Friday", "Saturday", "Sunday"]).astype(int)

# í•™ìŠµ ì‹œì ì˜ ì¸ì½”ë”ë¡œ ë³€í™˜ (ì£¼ì˜!)
try:
    new_data['dayname_encoded'] = le.transform(new_data['dayname'])
except ValueError:
    logger.warning("âš ï¸ ì‹ ê·œ ë°ì´í„°ì— í•™ìŠµ ì‹œì ì— ì—†ë˜ ìš”ì¼ì´ ìˆìŠµë‹ˆë‹¤. í™•ì¸ í•„ìš”.")

X = new_data[numeric_features + categorical_features]

# ============================================
# ğŸ“Š ì˜ˆì¸¡ ë° ê²°ê³¼ ë³‘í•©
# ============================================
preds = np.clip(model.predict(X), 0, None)
new_data["predicted_total"] = preds
new_data['grid_id'] = new_data['grid_id'].astype(int)
logger.info("âœ… ì˜ˆì¸¡ ì™„ë£Œ")

# ============================================
# ğŸ’¾ ê²°ê³¼ ì €ì¥
# ============================================
result_cols = ['grid_id', 'std_date', 'predicted_total', 'acs_cnt']
results = new_data[result_cols]
# í˜„ì¬ ì‹œê°ì„ reg_dttm ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
results["reg_dttm"] = datetime.now()  # í˜„ì¬ ì‹œê°
# ì»¬ëŸ¼ ìˆœì„œ ì¡°ì • (ì›í•˜ëŠ” ìˆœì„œë¡œ)
save_cols = result_cols + ["reg_dttm"]
results = results[save_cols]

results["grid_id"] = results["grid_id"].astype(str)
results.to_sql('tb_wifi_prediction', target_engine, schema='public', if_exists='append', index=False)
logger.info("âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ to TB_WIFI_PREDICTION")

if __name__ == "__main__":
    pass 
    #logger.info("ğŸš€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œì‘")