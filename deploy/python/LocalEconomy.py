import argparse
import pandas as pd
import json
import logging
import os
from sqlalchemy import create_engine
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# -----------------------------------------------------------
# ğŸª¶ ë¡œê¹… ì„¤ì •
# -----------------------------------------------------------
def setup_logger(script_name):
    log_dir = os.getenv("LOG_DIR", "./logs")  # ê¸°ë³¸ ë¡œê·¸ ë””ë ‰í† ë¦¬
    log_file_path = os.path.join(log_dir, f"{script_name}.log")  # íŒŒì¼ëª… ë™ì  ì„¤ì •

    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)

    logger = logging.getLogger(script_name)
    if not logger.handlers:  # ì¤‘ë³µ ë°©ì§€
        logging.basicConfig(
            filename=log_file_path,
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S"
        )
        console = logging.StreamHandler()
        console.setLevel(logging.INFO)
        formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
        console.setFormatter(formatter)
        logger.addHandler(console)
        logger.info("ğŸ“˜ Logging initialized.")
    return logger

# ------------------------------------------------------------------------
# PostgreSQL ì—°ê²° ìƒì„±
# ------------------------------------------------------------------------
def get_engine():
    db_config = {
        "DB_USER": os.getenv("DB_USER"),
        "DB_PASS": os.getenv("DB_PASS"),
        "DB_HOST": os.getenv("DB_HOST"),
        "DB_PORT": os.getenv("DB_PORT"),
        "DB_NAME": os.getenv("DB_NAME")
    }

    url = (
        f"postgresql+psycopg2://{db_config['DB_USER']}:{db_config['DB_PASS']}"
        f"@{db_config['DB_HOST']}:{db_config['DB_PORT']}/{db_config['DB_NAME']}"
    )
    return create_engine(url)

# ------------------------------------------------------------------------
# KCB ë°ì´í„° ì²˜ë¦¬ ë° ì ì¬
# ------------------------------------------------------------------------
def process_kcb(logger):
    logger.info("ğŸš€ KCB ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")

    # Data Loading
    kcb = pd.read_csv('data/YEOSU_SOHO_STAT_2001-2507.txt', sep='|')
    ind_code = pd.read_csv('data/YEOSU_IND_CODE.txt', sep='|')

    # Cleaning & Merge

    kcb = pd.merge(kcb, ind_code, left_on='SIC_CD_LV4', right_on='SIC_CD', how='inner').drop(columns='SIC_CD')
    drop_cols = [
        'WGS84_X', 'WGS84_Y', 'UTMK_X', 'UTML_Y',
        'RUN_OUT2_CNT', 'TOT_SALES_AMT1_CNT', 'TOT_SALES_AMT2_CNT',
        'TOT_SALES_AMT3_CNT', 'TOT_SALES_AMT4_CNT'
    ]
    
    kcb.drop(columns=drop_cols, inplace=True)
    kcb = kcb[[
        'QID50', 'BS_YR_MON', 'SIC_CD_LV4','SIC_FST_CLSFY_ITM_NM', 'SIC_SCND_CLSFY_ITM_NM','SIC_TRD_CLSFY_ITM_NM', 'SIC_FOUR_CLSFY_ITM_NM',
        'SHOP_CNT', 'OP_CNT', 'NEW_OPN_CNT', 'RUN_OUT_CNT', 'TOT_SALE_AMT', 'TOT_SALES_AMT0_CNT',  'TOT_SALES_AMT5_CNT', ]]


    logger.info(f"KCB ë°ì´í„° ì •ì œ ì™„ë£Œ: {kcb.shape[0]} rows, {kcb.shape[1]} columns")

    # PostgreSQL Insert
    engine = get_engine()
    kcb.to_sql(name='tb_kcb_stat', con=engine, if_exists='append', index=False, chunksize=10000, method='multi')

    logger.info("âœ… KCB ë°ì´í„° DB ì ì¬ ì™„ë£Œ")

# ------------------------------------------------------------------------
# Local Pay ë°ì´í„° ì²˜ë¦¬ ë° ì ì¬
# ------------------------------------------------------------------------
def process_local(logger):
    logger.info("ğŸš€ Local Pay ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")

    # Data Loading
    local_pay = pd.read_csv('data/local_pay_202501_09.csv')
    with open('data/json/local_grid_id.json', 'r', encoding='utf-8') as f:
        local_grid_id = json.load(f)

    # Cleaning
    local_pay['ê²°ì œë…„ì›”ì¼'] = pd.to_datetime(local_pay['ê²°ì œë…„ì›”ì¼'])
    local_pay['ê²°ì œë…„ì›”'] = local_pay['ê²°ì œë…„ì›”ì¼'].dt.strftime('%Y-%m')
    local_pay['grid_id'] = local_pay['ê°€ë§¹ì ëª…'].map(local_grid_id)

    local_pay['std_ym'] = pd.to_datetime(local_pay['ê²°ì œë…„ì›”']).dt.strftime("%Y%m")
    local_pay_agg = local_pay.groupby(['ì—…ì¢…', 'grid_id', 'std_ym'], as_index=False).agg(
        pay_cnt=('ë²ˆí˜¸', 'count'),
        pay_amt=('ê²°ì œê¸ˆì•¡', 'sum')
    )[['grid_id', 'std_ym', 'ì—…ì¢…', 'pay_cnt', 'pay_amt']]

    logger.info(f"Local Pay ì§‘ê³„ ì™„ë£Œ: {local_pay_agg.shape[0]} rows")

    # PostgreSQL Insert
    engine = get_engine()
    local_pay_agg.to_sql(name='tb_local_pay_agg', con=engine, if_exists='append', index=False, chunksize=10000, method='multi')

    logger.info("âœ… Local Pay ë°ì´í„° DB ì ì¬ ì™„ë£Œ")

# ------------------------------------------------------------------------
# Main Entry
# ------------------------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="KCB / Local Pay ë°ì´í„° ì²˜ë¦¬ ë° DB ì ì¬")
    parser.add_argument("--target", type=str, required=True, choices=["kcb", "local"], help="ì²˜ë¦¬í•  ë°ì´í„° ì¢…ë¥˜ ì„ íƒ")
    args = parser.parse_args()

    # ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„ì— ë”°ë¼ ë¡œê±° ì„¤ì •
    logger = setup_logger("LocalEconomy")
    logger.info(f"â–¶ ì‹¤í–‰ ëŒ€ìƒ: {args.target.upper()}")

    try:
        if args.target == "kcb":
            process_kcb(logger)
        elif args.target == "local":
            process_local(logger)
    except Exception as e:
        logger.exception(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")